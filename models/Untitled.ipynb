{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Member</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zain</td>\n",
       "      <td>Ok so all this server is a hub for friends to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zain</td>\n",
       "      <td>Do you have an Ikea?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zain</td>\n",
       "      <td>yee boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zain</td>\n",
       "      <td>Yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zain</td>\n",
       "      <td>Noice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Member                                            Message\n",
       "0   Zain  Ok so all this server is a hub for friends to ...\n",
       "1   Zain                               Do you have an Ikea?\n",
       "2   Zain                                            yee boy\n",
       "3   Zain                                               Yeah\n",
       "4   Zain                                              Noice"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/messages.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'gpt2'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df['Message'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18690,\n",
       " 523,\n",
       " 477,\n",
       " 428,\n",
       " 4382,\n",
       " 318,\n",
       " 257,\n",
       " 12575,\n",
       " 329,\n",
       " 2460,\n",
       " 284,\n",
       " 711,\n",
       " 1830,\n",
       " 11,\n",
       " 8537,\n",
       " 11,\n",
       " 8181,\n",
       " 448,\n",
       " 11,\n",
       " 290,\n",
       " 2429,\n",
       " 81,\n",
       " 453,\n",
       " 423,\n",
       " 1257,\n",
       " 13,\n",
       " 770,\n",
       " 4382,\n",
       " 318,\n",
       " 6229,\n",
       " 284,\n",
       " 307,\n",
       " 257,\n",
       " 9014,\n",
       " 329,\n",
       " 262,\n",
       " 8745,\n",
       " 1292,\n",
       " 21682,\n",
       " 37018,\n",
       " 4382,\n",
       " 326,\n",
       " 468,\n",
       " 1716,\n",
       " 845,\n",
       " 1588,\n",
       " 290,\n",
       " 7379,\n",
       " 1164,\n",
       " 88,\n",
       " 13,\n",
       " 383,\n",
       " 8745,\n",
       " 1292,\n",
       " 21682,\n",
       " 37018,\n",
       " 4382,\n",
       " 468,\n",
       " 635,\n",
       " 1716,\n",
       " 845,\n",
       " 28621,\n",
       " 290,\n",
       " 23196,\n",
       " 378,\n",
       " 355,\n",
       " 257,\n",
       " 1255,\n",
       " 286,\n",
       " 663,\n",
       " 2546,\n",
       " 13,\n",
       " 1320,\n",
       " 318,\n",
       " 1521,\n",
       " 716,\n",
       " 2911,\n",
       " 278,\n",
       " 284,\n",
       " 1394,\n",
       " 428,\n",
       " 2055,\n",
       " 5365,\n",
       " 1402,\n",
       " 523,\n",
       " 326,\n",
       " 356,\n",
       " 460,\n",
       " 307,\n",
       " 4075,\n",
       " 290,\n",
       " 423,\n",
       " 1257,\n",
       " 13]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(messages[0])\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok so all this server is a hub for friends to play games, chat, hangout, and genrally have fun. This server is ment to be a replacement for the orginal frostbite server that has become very large and unwieldy. The orginal frostbite server has also become very inactive and stagnate as a result of its size. That is why am hopeing to keep this community relatively small so that we can be active and have fun.\n"
     ]
    }
   ],
   "source": [
    "decode_result = tokenizer.decode(ids)\n",
    "print(decode_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 94, but ``max_length`` is set to 20.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "t = torch.LongTensor(ids)[None]\n",
    "preds = model.generate(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 94]),\n",
       " tensor([18690,   523,   477,   428,  4382,   318,   257, 12575,   329,  2460,\n",
       "           284,   711,  1830,    11,  8537,    11,  8181,   448,    11,   290,\n",
       "          2429,    81,   453,   423,  1257,    13,   770,  4382,   318,  6229,\n",
       "           284,   307,   257,  9014,   329,   262,  8745,  1292, 21682, 37018,\n",
       "          4382,   326,   468,  1716,   845,  1588,   290,  7379,  1164,    88,\n",
       "            13,   383,  8745,  1292, 21682, 37018,  4382,   468,   635,  1716,\n",
       "           845, 28621,   290, 23196,   378,   355,   257,  1255,   286,   663,\n",
       "          2546,    13,  1320,   318,  1521,   716,  2911,   278,   284,  1394,\n",
       "           428,  2055,  5365,  1402,   523,   326,   356,   460,   307,  4075,\n",
       "           290,   423,  1257,    13]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape,preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok so all this server is a hub for friends to play games, chat, hangout, and genrally have fun. This server is ment to be a replacement for the orginal frostbite server that has become very large and unwieldy. The orginal frostbite server has also become very inactive and stagnate as a result of its size. That is why am hopeing to keep this community relatively small so that we can be active and have fun.\n"
     ]
    }
   ],
   "source": [
    "prediction_result = tokenizer.decode(preds[0].numpy())\n",
    "print(prediction_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridging the gap with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.head(66488)\n",
    "df_val = df.tail(16622)\n",
    "all_texts = df['Message'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def encodes(self, x): \n",
    "        toks = self.tokenizer.tokenize(x)\n",
    "        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n",
    "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [range_of(df_train), list(range(len(df_train), len(all_texts)))]\n",
    "tls = TfmdLists(all_texts, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18690,   523,   477,   428,  4382,   318,   257, 12575,   329,  2460,\n",
       "           284,   711,  1830,    11,  8537,    11,  8181,   448,    11,   290,\n",
       "          2429,    81,   453,   423,  1257,    13,   770,  4382,   318,  6229,\n",
       "           284,   307,   257,  9014,   329,   262,  8745,  1292, 21682, 37018,\n",
       "          4382,   326,   468,  1716,   845,  1588,   290,  7379,  1164,    88,\n",
       "            13,   383,  8745,  1292, 21682, 37018,  4382,   468,   635,  1716,\n",
       "           845, 28621,   290, 23196,   378,   355,   257,  1255,   286,   663,\n",
       "          2546,    13,  1320,   318,  1521,   716,  2911,   278,   284,  1394,\n",
       "           428,  2055,  5365,  1402,   523,   326,   356,   460,   307,  4075,\n",
       "           290,   423,  1257,    13]),\n",
       " tensor([ 5171,   470,  2652,  2208,   890, 42796]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tls.train[0],tls.valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([94]), torch.Size([6]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tls.tfms(tls.train.items[0]).shape, tls.tfms(tls.valid.items[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok so all this server is a hub for friends to play games, chat, hangout, and genrally have fun. This server is ment to be a replacement for the orginal frostbite server that has become very large and unwieldy. The orginal frostbite server has also become very inactive and stagnate as a result of its size. That is why am hopeing to keep this community relatively small so that we can be active and have fun.\n"
     ]
    }
   ],
   "source": [
    "show_at(tls.train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't stay super long tho\n"
     ]
    }
   ],
   "source": [
    "show_at(tls.valid, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TfmdLists in module fastai.data.core object:\n",
      "\n",
      "class TfmdLists(FilteredBase, fastcore.foundation.L, fastcore.foundation.GetAttr)\n",
      " |  TfmdLists(items=None, *rest, use_list=False, match=None)\n",
      " |  \n",
      " |  A `Pipeline` of `tfms` applied to a collection of `items`\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TfmdLists\n",
      " |      FilteredBase\n",
      " |      fastcore.foundation.L\n",
      " |      fastcore.foundation.GetAttr\n",
      " |      fastcore.foundation.CollBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, o, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __getitem__(self, idx)\n",
      " |      Retrieve `idx` (can be list of indices, or mask, or int) items\n",
      " |  \n",
      " |  __init__(self, items, tfms, use_list=None, do_setup=True, split_idx=None, train_setup=True, splits=None, types=None, verbose=False, dl_type=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  decode(self, o, **kwargs)\n",
      " |      From `Pipeline`\n",
      " |  \n",
      " |  infer(self, x)\n",
      " |      Apply `self.tfms` to `x` starting at the right tfm depending on the type of `x`\n",
      " |  \n",
      " |  infer_idx(self, x)\n",
      " |      Finds the index where `self.tfms` can be applied to `x`, depending on the type of `x`\n",
      " |  \n",
      " |  new_empty(self)\n",
      " |      A new version of `self` but with no items\n",
      " |  \n",
      " |  overlapping_splits(self)\n",
      " |      All splits that are in more than one split\n",
      " |  \n",
      " |  setup(self, train_setup=True)\n",
      " |      Transform setup with self\n",
      " |  \n",
      " |  show(self, o, **kwargs)\n",
      " |      From `Pipeline`\n",
      " |  \n",
      " |  subset(self, i)\n",
      " |      New `TfmdLists` with same tfms that only includes items in `i`th split\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from FilteredBase:\n",
      " |  \n",
      " |  dataloaders(self, bs=64, val_bs=None, shuffle_train=True, n=None, path='.', dl_type=None, dl_kwargs=None, device=None, shuffle=False, num_workers=None, verbose=False, do_setup=True, pin_memory=False, timeout=0, batch_size=None, drop_last=False, indexed=None, persistent_workers=False, *, wif=None, before_iter=None, after_item=None, before_batch=None, after_batch=None, after_iter=None, create_batches=None, create_item=None, create_batch=None, retain=None, get_idxs=None, sample=None, shuffle_fn=None, do_batch=None)\n",
      " |      Get a `DataLoaders`\n",
      " |  \n",
      " |  partial_dataloaders(self: fastai.data.core.FilteredBase, partial_n, bs=64, val_bs=None, shuffle_train=True, n=None, path='.', dl_type=None, dl_kwargs=None, device=None)\n",
      " |      Create a partial dataloader `PartialDL` for the training set\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from FilteredBase:\n",
      " |  \n",
      " |  n_subsets\n",
      " |  \n",
      " |  train\n",
      " |      partial(func, *args, **keywords) - new function with partial application\n",
      " |      of the given arguments and keywords.\n",
      " |  \n",
      " |  valid\n",
      " |      partial(func, *args, **keywords) - new function with partial application\n",
      " |      of the given arguments and keywords.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from FilteredBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from fastcore.foundation.L:\n",
      " |  \n",
      " |  __add__(a, b)\n",
      " |  \n",
      " |  __addi__(a, b)\n",
      " |  \n",
      " |  __contains__(self, b)\n",
      " |  \n",
      " |  __eq__(self, b)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __invert__(self)\n",
      " |  \n",
      " |  __mul__(a, b)\n",
      " |  \n",
      " |  __radd__(a, b)\n",
      " |  \n",
      " |  __reversed__(self)\n",
      " |  \n",
      " |  __setitem__(self, idx, o)\n",
      " |      Set `idx` (can be list of indices, or mask, or int) items to `o` (which is broadcast if not iterable)\n",
      " |  \n",
      " |  argwhere(self, f, negate=False, **kwargs)\n",
      " |      Like `filter`, but return indices for matching items\n",
      " |  \n",
      " |  attrgot(self, k, default=None)\n",
      " |      Create new `L` with attr `k` (or value `k` for dicts) of all `items`.\n",
      " |  \n",
      " |  cat(self: fastcore.foundation.L, dim=0)\n",
      " |      Same as `torch.cat`\n",
      " |  \n",
      " |  concat(self)\n",
      " |      Concatenate all elements of list\n",
      " |  \n",
      " |  copy(self)\n",
      " |      Same as `list.copy`, but returns an `L`\n",
      " |  \n",
      " |  cycle(self)\n",
      " |      Same as `itertools.cycle`\n",
      " |  \n",
      " |  enumerate(self)\n",
      " |      Same as `enumerate`\n",
      " |  \n",
      " |  filter(self, f=<function noop at 0x7f03fa532c10>, negate=False, gen=False, **kwargs)\n",
      " |      Create new `L` filtered by predicate `f`, passing `args` and `kwargs` to `f`\n",
      " |  \n",
      " |  itemgot(self, *idxs)\n",
      " |      Create new `L` with item `idx` of all `items`\n",
      " |  \n",
      " |  map(self, f, *args, gen=False, **kwargs)\n",
      " |      Create new `L` with `f` applied to all `items`, passing `args` and `kwargs` to `f`\n",
      " |  \n",
      " |  map_dict(self, f=<function noop at 0x7f03fa532c10>, *args, gen=False, **kwargs)\n",
      " |      Like `map`, but creates a dict from `items` to function results\n",
      " |  \n",
      " |  map_first(self, f=<function noop at 0x7f03fa532c10>, g=<function noop at 0x7f03fa532c10>, *args, **kwargs)\n",
      " |      First element of `map_filter`\n",
      " |  \n",
      " |  map_zip(self, f, *args, cycled=False, **kwargs)\n",
      " |      Combine `zip` and `starmap`\n",
      " |  \n",
      " |  map_zipwith(self, f, *rest, cycled=False, **kwargs)\n",
      " |      Combine `zipwith` and `starmap`\n",
      " |  \n",
      " |  product(self)\n",
      " |      Product of the items\n",
      " |  \n",
      " |  reduce(self, f, initial=None)\n",
      " |      Wrapper for `functools.reduce`\n",
      " |  \n",
      " |  renumerate(self)\n",
      " |      Same as `renumerate`\n",
      " |  \n",
      " |  setattrs(self, attr, val)\n",
      " |      Call `setattr` on all items\n",
      " |  \n",
      " |  shuffle(self)\n",
      " |      Same as `random.shuffle`, but not inplace\n",
      " |  \n",
      " |  sorted(self, key=None, reverse=False)\n",
      " |      New `L` sorted by `key`. If key is str use `attrgetter`; if int use `itemgetter`\n",
      " |  \n",
      " |  stack(self: fastcore.foundation.L, dim=0)\n",
      " |      Same as `torch.stack`\n",
      " |  \n",
      " |  starmap(self, f, *args, **kwargs)\n",
      " |      Like `map`, but use `itertools.starmap`\n",
      " |  \n",
      " |  sum(self)\n",
      " |      Sum of the items\n",
      " |  \n",
      " |  tensored(self: fastcore.foundation.L)\n",
      " |      `mapped(tensor)`\n",
      " |  \n",
      " |  unique(self, sort=False, bidir=False, start=None)\n",
      " |      Unique items, in stable order\n",
      " |  \n",
      " |  val2idx(self)\n",
      " |      Dict from value to index\n",
      " |  \n",
      " |  zip(self, cycled=False)\n",
      " |      Create new `L` with `zip(*items)`\n",
      " |  \n",
      " |  zipwith(self, *rest, cycled=False)\n",
      " |      Create new `L` with `self` zip with each of `*rest`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from fastcore.foundation.L:\n",
      " |  \n",
      " |  range(a, b=None, step=None) from fastcore.foundation._L_Meta\n",
      " |      Class Method: Same as `range`, but returns `L`. Can pass collection for `a`, to use `len(a)`\n",
      " |  \n",
      " |  split(s, sep=None, maxsplit=-1) from fastcore.foundation._L_Meta\n",
      " |      Class Method: Same as `str.split`, but returns an `L`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from fastcore.foundation.L:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __signature__ = <Signature (items=None, *rest, use_list=False, match=N...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from fastcore.foundation.GetAttr:\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, k)\n",
      " |  \n",
      " |  __setstate__(self, data)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from fastcore.foundation.CollBase:\n",
      " |  \n",
      " |  __delitem__(self, i)\n",
      " |  \n",
      " |  __len__(self)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-085b1158bedd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dls' is not defined"
     ]
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
